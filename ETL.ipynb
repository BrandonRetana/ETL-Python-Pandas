{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2498e772",
   "metadata": {},
   "source": [
    "La siguiente celda se encargara de importar todos los paquetes necesarios para el desarrollo del proyecto. Además, se importa el archivo de configuración de la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d3e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import ltrim,rtrim,trim,col\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.5.1.jar\") \\\n",
    "\t.master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3875913",
   "metadata": {},
   "source": [
    "En la siguiente celda se encarga de utilizar spark para cargar en dos diferentes dataframes los datos del INEC y el OIJ para despues limpiarlos y unirlos en un solo dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5739b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE DATAFRAME FROM CSV FILE\n",
    "oij_df = spark.read.csv( path=\"OIJ.csv\", sep=\",\", header=True,quote='\"',inferSchema=True,)\n",
    "inec_df = spark.read.csv( path=\"INEC.csv\", sep=\";\", header=True,quote='\"',inferSchema=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379181",
   "metadata": {},
   "source": [
    "En la siguiente celda se encarga de realizar la limpieza de los datos del INEC y el OIJ. Primero se creo una función para remover todos los espacios en blanco al inicio y al final de cada sub celda del data frame utilizando la funcion trim para ello. Ademas se creo una funcio la cual se encarga de pasar todos los caracteres a letras minusculas para poder realizar la busqueda de los datos en el data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfe503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove spaces from the beginning and end of the string\n",
    "def remove_spaces(df):\n",
    "    for col in df.columns:\n",
    "        if  col == 'Provincia' or col == 'Canton' or col == 'Distrito' or col == 'Provincia, cantón y distrito':\n",
    "            df = df.withColumn(col, trim(col))\n",
    "    return df\n",
    "\n",
    "#Function to parser the string to lowercase\n",
    "def to_lower_case(df):\n",
    "    for col in df.columns:\n",
    "        if  col == 'Provincia' or col == 'Canton' or col == 'Distrito' or col == 'Provincia, cantón y distrito':\n",
    "            df = df.withColumn(col, lower(col))\n",
    "    return df\n",
    "\n",
    "\n",
    "oij_df = remove_spaces(oij_df)\n",
    "inec_df = remove_spaces(inec_df)\n",
    "\n",
    "oij_df = to_lower_case(oij_df)\n",
    "inec_df = to_lower_case(inec_df)\n",
    "\n",
    "oij_df.show(5)\n",
    "inec_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c623bb",
   "metadata": {},
   "source": [
    "En la siguente celda se encuentra una funcion la cual se encarga de revisiar en los dos data frames aquellos datos donde no coinciden los datos de la provincia, canton y distrito. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function find list the not match values in the dataframes\n",
    "def find_non_matches(df1,df2):\n",
    "    non_matches = []\n",
    "    for col in df1.columns:\n",
    "        if  col == 'Provincia' or col == 'Canton' or col == 'Distrito':\n",
    "            for row in df1.select(col).distinct().collect():\n",
    "                if not df2.filter(df2['Provincia, cantón y distrito'] == row[col]).collect():\n",
    "                    non_matches.append(row[col])\n",
    "    return non_matches\n",
    "\n",
    "non_matches_oij = find_non_matches(oij_df,inec_df)\n",
    "\n",
    "\n",
    "\n",
    "oij_df = remove_spaces(oij_df)\n",
    "inec_df = remove_spaces(inec_df)\n",
    "\n",
    "oij_df = to_lower_case(oij_df)\n",
    "inec_df = to_lower_case(inec_df)\n",
    "\n",
    "nonMatches = find_non_matches(oij_df,inec_df)\n",
    "print(nonMatches)\n",
    "#oij_df.show(5)\n",
    "#inec_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16b2ba",
   "metadata": {},
   "source": [
    "En la siguiente celda se encarga de reemplazar todos los caracteres especiales del español por caracteres ascii para poder realizar la busqueda de los datos en el data frame. Y que su taza de concidencia sea mayor.Para este trabajo se utilizo la funcionalidad regex_replace de spark. Y asi cada vez que se encuentre un caracter especial se reemplazara por un caracter ascii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3046964e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to replace the accents in column Provincia, cantón y distrito in inec_df\n",
    "def replace_accents(df):\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'á', 'a'))\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'é', 'e'))\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'í', 'i'))\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'ó', 'o'))\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'ú', 'u'))\n",
    "    df = df.withColumn('Provincia, cantón y distrito', regexp_replace('Provincia, cantón y distrito', 'ñ', 'n'))\n",
    "    return df\n",
    "\n",
    "inec_df = replace_accents(inec_df)\n",
    "nonMatches = find_non_matches(oij_df,inec_df)\n",
    "print(nonMatches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55c12f7",
   "metadata": {},
   "source": [
    "En la siguiente celda se encarga de dividir la columna del dataframe del inec en tres columnas diferentes, una para la provincia, otra para el canton y otra para el distrito. Para esto se creo una funcion que basado en la cantidad espacios que se encuentren en la columna se dividira en tres columnas diferentes. Esto para que la comparacion de los datos sea mas exacta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_columns(df):\n",
    "    columns = [\"Provincia1\", \"Canton1\", \"Distrito1\",\"Tasa neta de participación\", \"Porcentaje de población económicamente inactiva\", \"Relación de dependencia económica\"]\n",
    "    new_df = spark.createDataFrame(data =[(\"\",\"\",\"\",\"\",\"\",\"\")], schema = columns)\n",
    "    Provincia =''\n",
    "    Canton = ''\n",
    "    Distrito = ''\n",
    "    counter = 0\n",
    "    counter2 = 0\n",
    "    for row in df.collect():\n",
    "        if row['Provincia, cantón y distrito'] == None:\n",
    "            counter += 1\n",
    "            counter2 += 1\n",
    "            if counter == 4: \n",
    "                counter = 2  \n",
    "\n",
    "            if counter2 == 2:\n",
    "                counter = 1\n",
    "            continue\n",
    "        if counter == 1:\n",
    "            Provincia = row[0]\n",
    "        if counter == 2:\n",
    "            Canton = row[0]\n",
    "        if counter == 3:\n",
    "            Distrito = row[0]\n",
    "            NewRow = (Provincia, Canton, Distrito, \"\", \"\", \"\")\n",
    "            new_df = new_df.union(spark.createDataFrame(data =[NewRow], schema = columns))\n",
    "        counter2 = 0\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "new_df = generate_new_columns(inec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085bc82",
   "metadata": {},
   "source": [
    "Esta funcion todavia no se si se deberia utilizar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162d76ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239e2ce8",
   "metadata": {},
   "source": [
    "En la siguiente celda se encarga de la conexion a la base de datos y la creacion de las tablas necesarias para el almacenamiento de los datos ya limpios y procesados. Se crean dos tablas una para el inec y otra para el oij."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebda3932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sql table from dataframe\n",
    "url = \"jdbc:postgresql://localhost:5432/etl\"\n",
    "mode = \"overwrite\"\n",
    "properties = {\"user\": \"postgres\", \" password\": \"Legolas00\", \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "new_df.write.jdbc(url=url, table=\"INEC\", mode=mode, properties=properties)\n",
    "oij_df.write.jdbc(url=url, table=\"OIJ\", mode=mode, properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89b29b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
